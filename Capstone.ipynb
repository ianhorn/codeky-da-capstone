{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Flooding Areas from NRT Sentinel-1 Satellite Imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting folium\n",
      "  Using cached folium-0.19.5-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: branca>=0.6.0 in ./venv/lib/python3.13/site-packages (from folium) (0.8.1)\n",
      "Requirement already satisfied: jinja2>=2.9 in ./venv/lib/python3.13/site-packages (from folium) (3.1.6)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.13/site-packages (from folium) (2.1.3)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.13/site-packages (from folium) (2.32.3)\n",
      "Requirement already satisfied: xyzservices in ./venv/lib/python3.13/site-packages (from folium) (2025.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.13/site-packages (from jinja2>=2.9->folium) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests->folium) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.13/site-packages (from requests->folium) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.13/site-packages (from requests->folium) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.13/site-packages (from requests->folium) (2025.1.31)\n",
      "Using cached folium-0.19.5-py2.py3-none-any.whl (110 kB)\n",
      "Installing collected packages: folium\n",
      "Successfully installed folium-0.19.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install pandas\n",
    "# %pip install geopandas\n",
    "# %pip install dataretrieval\n",
    "# %pip install matplotlib\n",
    "# %pip install ipyleaflet\n",
    "# %pip install ipywidgets\n",
    "# %pip install hydrosar\n",
    "# %pip install asf_tools\n",
    "# %pip install folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd                 \n",
    "import geopandas as gpd                                             # read geospatial data\n",
    "import dataretrieval.nwis as nwis  \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import subprocess                                                   # to run commandline commands\n",
    "from ipyleaflet import Map, basemaps, basemap_to_tiles , TileLayer\n",
    "from ipyleaflet import GeoData, LayersControl, ImageOverlay\n",
    "import ipywidgets as widgets                                        # interactive display\n",
    "from hydrosar.flood_map import make_flood_map\n",
    "import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review the water data\n",
    "\n",
    "Use the [USGS Water Services Instantaneous API](https://waterservices.usgs.gov/test-tools/?service=iv&siteType=&statTypeCd=all&major-filters=sites&format=json&date-type=type-none&statReportType=daily&statYearType=calendar&missingData=off&siteStatus=all&siteNameMatchOperator=start) to retreive data for a USGS Gage in Frankfort, Kentucky. \n",
    "\n",
    "Dataretrieval Python Documentation:\n",
    "    [https://doi-usgs.github.io/dataretrieval-python/index.html](https://doi-usgs.github.io/dataretrieval-python/index.html)  \n",
    "    I will used the dataretrievel.nwis.get_iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up variables for the instantaneous values (iv)\n",
    "# query ~60 days of data\n",
    "sites= '03287500'                      # site number for USGS Gage in Frankfort\n",
    "start = '2025-01-15'                   # filter for the range of start date\n",
    "end = '2025-03-15'                     # and end data\n",
    "parameterCd = [\"00060\", \"00065\"]       # parameter codes for gage discharge in cubic feet/second\n",
    "                                       # and height in feet                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a metadata link and dataframe\n",
    "water_df, water_md = nwis.get_iv(\n",
    "    sites=sites,\n",
    "    start=start,\n",
    "    end=end,\n",
    "    parameterCd=parameterCd,\n",
    "    multi_index=True,\n",
    ")\n",
    "print(f'Metadata Link: {water_md}\\n')\n",
    "print(f'water_df.columns\\n')\n",
    "print(f'The stream gage table has {water_df.shape[1]} rows and {water_df.shape[0]} colummns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe cleaning\n",
    "- drop columns\n",
    "- rename columns\n",
    "- reset index\n",
    "- convert datatime to date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_water_df(df):\n",
    "    # Drop site_no and columns that end in cd\n",
    "    cols_to_drop = ['site_no','00060_cd', '00065_cd']\n",
    "    df = df.drop(columns=cols_to_drop, axis=1)\n",
    "\n",
    "    # rename columns\n",
    "    cols_to_rename = {'00060': 'discharge (cubic feet)', '00065': 'stream height (feet)'}\n",
    "    df = df.rename(columns=cols_to_rename)\n",
    "\n",
    "    # reset index so we can use datetime\n",
    "    df = df.reset_index()           \n",
    "    df['date'] = pd.to_datetime(df['datetime'], errors='coerce').dt.date\n",
    "    df\n",
    "\n",
    "    new_df = df.iloc[:, [3, 2, 1]]\n",
    "    new_df\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call function to clean\n",
    "clean_water_df = clean_water_df(water_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\n{clean_water_df.dtypes}')         # check dtype\n",
    "clean_water_df.head(10)                      # See actual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group the data by date and return max value for stream height and discharge for that day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_water_df_groupby = clean_water_df.groupby('date', as_index=False).agg({\n",
    "    'stream height (feet)': 'max',\n",
    "    'discharge (cubic feet)': 'max'\n",
    "})\n",
    "print(f'The new dataframe has {clean_water_df_groupby.shape[1]} columns and {clean_water_df_groupby.shape[0]} rows.')\n",
    "clean_water_df_groupby.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gage_data = clean_water_df_groupby\n",
    "\n",
    "# have the plot read date as datetime, otherwise it looks messy\n",
    "gage_data['date'] = pd.to_datetime(gage_data['date'])\n",
    "\n",
    "# set x and y values\n",
    "x = gage_data['date']\n",
    "y1 = gage_data['stream height (feet)']\n",
    "y2 = gage_data['discharge (cubic feet)']\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "ax.plot(x, y1, color = 'b', label='Stream Height (feet)')\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(x,y2, color='r', label='Discharge (cubic feet)')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_title('Stream Height & Discharge Over Time')\n",
    "\n",
    "# Add legends\n",
    "ax.legend(loc='upper left')  \n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# rotate the x-axis labels to fit more\n",
    "ax.tick_params(axis='x', rotation=45)  \n",
    "\n",
    "ax.grid(True)\n",
    "\n",
    "# plot\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Download Imagery\n",
    "\n",
    "1. Create a function that downloads two scenes and processing elevation data.\n",
    "2. Create a function that setups up a dataframe\n",
    "3. Create dataframe and download data\n",
    "\n",
    "\n",
    "*\\*This is for demonstration puruposed only.  You should have already downloaded the data\\**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download function\n",
    "def box_download(df):\n",
    "    out_folder = \"data\"\n",
    "    download_zip = os.path.join(out_folder, 'file.zip')\n",
    "    for index, row in df.iterrows():\n",
    "        link = row['link']\n",
    "        unzipped_folder = row['scene_folder']\n",
    "        unzip_path = os.path.join(out_folder, unzipped_folder)\n",
    "\n",
    "        cmd = f\"curl -L -o {download_zip} {link} && unzip {download_zip} -d {out_folder}\"\n",
    "        \n",
    "        if not os.path.exists(unzip_path):\n",
    "            subprocess.run(cmd, shell=True, check=True)\n",
    "            # delete downloaded zip file.\n",
    "            os.remove(download_zip)\n",
    "        else:\n",
    "            print(f'Extracted Files already exist for \"{unzipped_folder}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe function\n",
    "def create_scenes_df():\n",
    "    \n",
    "    \"\"\"\n",
    "       SAR Data:\n",
    "        These files were obtained using the ASF Vertex Data Search.\n",
    "        They were submitted for Radio Terrain Correctiom.\n",
    "        radiometry: gamma0\n",
    "        scale: power\n",
    "        pixel spacing: 10m\n",
    "       \n",
    "        Digital Elevation Model processing:\n",
    "         The dem processing folder contains the DEM used for radio-\n",
    "         terrain correction and derivative products used to create \n",
    "         a HAND and water mask.\n",
    "    \"\"\"\n",
    "\n",
    "    box_data = {\n",
    "        'zipfile': ['S1A_IW_20250124T233956_DVP_RTC10_G_gdufem_8285.zip',                        # Data ordered but not downloading\n",
    "                    'S1A_IW_20250217T233955_DVP_RTC10_G_gpufem_0598',\n",
    "                    'dem_processing.zip'],\n",
    "        'link': [\n",
    "                 'https://ky.box.com/shared/static/lchrmjmb5cy1jdtsq5wfp5y3hao5cs0d.zip',        # to keep within project scope\n",
    "                 'https://ky.box.com/shared/static/rz15mqljj1m0tcejhu647haey6sfrfkf.zip',\n",
    "                 'https://ky.box.com/s/rz15mqljj1m0tcejhu647haey6sfrfkf']\n",
    "            }\n",
    "    scenes_df = pd.DataFrame(data=box_data)\n",
    "\n",
    "    # calculate some fields\n",
    "    scenes_df['scene_folder'] = scenes_df['zipfile'].apply(lambda x: os.path.splitext(x)[0])\n",
    "    if not scenes_df['scene_folder'].eq('dem_processing.zip').any():\n",
    "        scenes_df['input_dem'] = scenes_df['scene_folder'] + '_dem.tif'\n",
    "        scenes_df['rgb'] = scenes_df['scene_folder'] +  '_rgb.png'\n",
    "        scenes_df['vv'] = scenes_df['scene_folder'] + '_vv.tif'\n",
    "        scenes_df['vh'] = scenes_df['scene_folder'] + '_vh.tif'\n",
    "        scenes_df['shapefile'] = scenes_df['scene_folder'] + '_shape.shp'\n",
    "    else:\n",
    "        print(\"dem processing is not a scene\")\n",
    "    return scenes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes_df = create_scenes_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data download from Box.  You shoud use the data [download link](README.md#data-collection-and-loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data from Box\n",
    "# make sure the data folder is present\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "    box_download(scenes_df)\n",
    "else:\n",
    "    print('data folder already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the on demand products download, I'm removing the `base_scene` because it will only be used for visualization.  I did export a few PNG files to have for comparison with the crisis images.  removing this folder will shorten download times considerably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_scene = scenes_df.at[0,'scene_folder']\n",
    "\n",
    "flood_scene = scenes_df.at[1,'scene_folder']\n",
    "dem_folder = scenes_df.at[2, 'scene_folder']\n",
    "print(f'Base Scene Directory: {flood_scene}\\nDEM products Directory: {dem_folder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in base image shapefile\n",
    "aoi_shapefile = (f'data/{flood_scene}/{flood_scene}_shape.shp')\n",
    "\n",
    "# read in shapefile to geodataframe\n",
    "gdf_base_shapefile = gpd.read_file(aoi_shapefile)\n",
    "print(f'source projection: {gdf_base_shapefile.crs}')\n",
    "\n",
    "# redefine the projection to work with the map\n",
    "gdf_base_shapefile = gdf_base_shapefile.to_crs('EPSG:4326')\n",
    "print(f'redefined projection: {gdf_base_shapefile.crs}')\n",
    "\n",
    "base_geo_data = GeoData(geo_dataframe = gdf_base_shapefile,\n",
    "                   style={'color': 'black', 'fillColor': '#3366cc', 'opacity':0.1, 'weight':1.9, 'dashArray':'2', 'fillOpacity':0.6},\n",
    "                   hover_style={'fillColor': 'red' , 'fillOpacity': 0.2},\n",
    "                   name = 'base Geometry')\n",
    "\n",
    "# read a Kentucky layer\n",
    "tcm_layer = TileLayer(\n",
    "    url = 'https://kygisserver.ky.gov/arcgis/rest/services/WGS84WM_Services/Ky_TCM_Base_WGS84WM/MapServer/tile/{z}/{y}/{x}',\n",
    "    attibution = 'The Commonwealth Basemap'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an ipyleaflet map\n",
    "center = [38.13707656, -85.31012789]\n",
    "zoom = 6.5\n",
    "leaflet_map = Map(\n",
    "    basemap=basemap_to_tiles(basemaps.CartoDB.Positron),\n",
    "    center=center,\n",
    "    zoom=zoom\n",
    ")\n",
    "leaflet_map.add(tcm_layer)\n",
    "leaflet_map.add(base_geo_data)\n",
    "control = LayersControl(position='topright')\n",
    "leaflet_map.add(control)\n",
    "leaflet_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place images in a variable\n",
    "png_folder = 'data/png_viz'\n",
    "composite_base = os.path.join(png_folder, 'composite_base.png')\n",
    "composite_crisis = os.path.join(png_folder, 'composite_crisis.png')\n",
    "\n",
    "image_list = [composite_base, composite_crisis]\n",
    "\n",
    "# plot images\n",
    "for c in image_list:\n",
    "    image = mpimg.imread(c)\n",
    "    print(f'Image: {os.path.basename(c)}')\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Flood Mapping\n",
    "\n",
    "Due to time constraints (i.e, processing imagery takes time), some processes like the Height Above Natural Drainage *HAND*, clipping the Digital Elevation Model *DEM* to the scene extent, and creating a water extent map were created using raster calulation functions in ArcGIS Pro and QGIS. I also downloaded [SNAP](https://step.esa.int/main/download/snap-download/) to understand better some of the routine transformations.\n",
    "\n",
    " In addition, some issues arose with the scenes themselves because of how I used the Vertex OnDemand Processing.  To not get too into the details, this had to do with transforming the raster scale from power to decibel.  I ordered decibel, which is what the final calculations call for, but the HydroSAR module expects power and peforms those transformations in its water map raster creation.  I was able to calculate the values to create the water extent in QGIS. This file [water_extent.tif](data/dem_processing/water_extent.tif) was placed in the dem processing folder and used in the `make_flood_map()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up flood processing\n",
    "- variables\n",
    "- create flood map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we have somewhere to park the processed data\n",
    "flood_processing = 'data/crisis_scene_processing'\n",
    "if not os.path.exists(flood_processing):\n",
    "    os.makedirs(flood_processing)\n",
    "\n",
    "scene_folder = os.path.join('data', flood_scene)\n",
    "dem_processing = 'data/dem_processing'\n",
    "\n",
    "# flood map variables\n",
    "flood_raster = os.path.join(flood_processing, 'flood_map.tif')\n",
    "vv_raster = os.path.join(scene_folder, scenes_df['vv'][1])\n",
    "water_extent_raster = os.path.join(dem_processing, 'water_extent.tif')\n",
    "hand_raster = f'{dem_processing}/hand.tif'\n",
    "hand_clipped = os.path.join(dem_processing, 'hand_clip.tif')\n",
    "flood_mask = os.path.join(flood_processing, 'flood_map_iterative_FloodMask.tif')  # Product of make_flood_map_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Flood map function, took nearly 15 minutes on my Mac Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run make_flood_map function\n",
    "if not os.path.exists(flood_mask):\n",
    "    make_flood_map(\n",
    "        flood_raster,\n",
    "        vv_raster,\n",
    "        water_extent_raster,\n",
    "        hand_raster\n",
    "    )\n",
    "else:\n",
    "    print('Make Flood Map Complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some layers to a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "![Flood Depht](media/flood_depth.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
